{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\nimport tensorflow.python.keras.layers\nfrom keras.preprocessing.image import ImageDataGenerator\n\n#Model Imports\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\n\n#Adding class weights\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#Adding Oversampler\nfrom imblearn.over_sampling import RandomOverSampler\n\n#Plotting Imports\nimport matplotlib.pyplot as plt\n\n#Confusion matrix import\nfrom sklearn.metrics import confusion_matrix\n\n#Train test split import\nfrom sklearn.model_selection import train_test_split\n\n#Import to calculate AUC\nfrom sklearn.metrics import roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import Data Sets\n","metadata":{}},{"cell_type":"code","source":"#Import the training dataset\ndataPath = \"/kaggle/input/c/siim-isic-melanoma-classification/train.csv\"\ndata_df = pd.read_csv(dataPath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ouput the head of the data datafreame\ndata_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop Duplicates, people on kaggle complained about duplicate image names.\n","metadata":{}},{"cell_type":"code","source":"#Drop duplicates based on \"image_name\" column\ndata_df = data_df.drop_duplicates(subset=['image_name'])\n\ndata_df = data_df.reindex()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data into a 20% testing dataframe,","metadata":{}},{"cell_type":"code","source":"#Set the test data to be 20% of the total data.\ntest_size = len(data_df) * 0.2\n\n#Get the test data from the data df\ntest_df = data_df.sample(n=int(test_size))\n\n#Output the size of the test data\nprint(\"The length of the testing data is: \" + str(len(test_df)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show the testing data frame\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the training dataframe.\n","metadata":{}},{"cell_type":"code","source":"#Create the training dataframe, set it to the data_df\ntrain_df = data_df\n\n#Drop all of the testing data\ntrain_df.drop(test_df.index, inplace=True)\n\n#Output the size of the training data\nprint(\"The length of the training data is: \" + str(len(train_df)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change the type so that it can be used in the ImageDataGenerator.\n","metadata":{}},{"cell_type":"code","source":"train_df['target'] = train_df['target'].astype(str)\ntest_df['target'] = test_df['target'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the ImagePath column, so that the images can be found within the ImageDataGenerator.\n","metadata":{}},{"cell_type":"code","source":"#Create a column for the full image path\ntrain_df['image_path'] = '/kaggle/input/c/siim-isic-melanoma-classification/jpeg/train/' + train_df['image_name'] +'.jpg'\ntest_df['image_path'] = '/kaggle/input/c/siim-isic-melanoma-classification/jpeg/train/' + test_df['image_name'] +'.jpg'\n\n#Show head of dataframe\ntrain_df.head()\ntrain_df['image_path']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the ImageDataGenerators for training,testing and validation.","metadata":{}},{"cell_type":"code","source":"# Initialize the ImageDataGenerator object\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)\n\n#Defining batch size\nbatch_size = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a training generator\ntraining_generator = datagen.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='image_path',\n    y_col='target',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training'\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a validation generator\nvalidation_generator = datagen.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='image_path',\n    y_col='target',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create the testing generator\ntesting_generator = datagen.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='image_path',\n    y_col='target',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the base model and download the weights.","metadata":{}},{"cell_type":"code","source":"#Define the image shape\nimage_shape = (224, 224, 3)\n\n#Weights pathway\nvgg_weights = '/kaggle/input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n#Define the base model\nbase_model = VGG16(weights=vgg_weights, include_top=False, input_shape=image_shape, classes =2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating the first Model.","metadata":{}},{"cell_type":"code","source":"#Define model parameters\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculating the training and validation steps.\n","metadata":{}},{"cell_type":"code","source":"#Calculate training steps\ntrain_steps = len(training_generator) // batch_size\n\n#Calculate the validation steps\nval_steps = len(validation_generator) // batch_size\n\n#Define number of epochs\nnum_epochs = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the first model.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save the model\n# save the model architecture to a JSON file\nmodel_json = model.to_json()\nwith open('my_model.json', 'w') as json_file:\n    json_file.write(model_json)\n\n# save the model weights to an HDF5 file\nmodel.save_weights('my_model_weights.h5')\nmodel.save('my_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model.evaluate(testing_generator, verbose=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned v alues\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create confusion matrix","metadata":{}},{"cell_type":"code","source":"model_predictions = model.predict(testing_generator)\n\n#Convert predicted probabilities to class labels\npredicted_labels = np.argmax(model_predictions, axis=1)\n\n#Extract true class labels from testing data\ntrue_labels = testing_generator.classes\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate confusion matrix\nmodel_confusion_matrix = confusion_matrix(true_labels, predicted_labels)\n\nprint(\"Confusion Matrix:\")\nprint(model_confusion_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model correctly predicted the negative class (benign) in all 1961 cases (true negative), but incorrectly predicted the positive class (malignant) in all 26 cases (false negative), with no false positives (predicted positive but true negative) or true positives (predicted positive and true positive). This suggests that the model has a relatively high specificity (ability to correctly identify the negative class) but a low sensitivity (ability to correctly identify the positive class)","metadata":{}},{"cell_type":"code","source":"#Plotting validation and training accuracy\nplt.plot(history.history['accuracy'], label='Training accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation accuracy')\nplt.title('Model accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a loss curve\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.title('Model loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting validation and training accuracy\nplt.plot(history.history['auc'], label='Training AUC')\nplt.plot(history.history['val_auc'], label='Validation AUC')\nplt.title('Model AUC')\nplt.xlabel('Epoch')\nplt.ylabel('AUC')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_df['target']), y=train_df['target'])\n\n# convert to dictionary\nclass_weight_dict = dict(enumerate(class_weights))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualise the class weights.\nclass_labels = np.unique(train_df['target'])\nplt.bar(class_labels, class_weights)\nplt.title('Class weights')\nplt.xlabel('Class label')\nplt.ylabel('Weight')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the second model using the class weights.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps, class_weight=class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save the model\n# save the model architecture to a JSON file\nmodel_json = model.to_json()\nwith open('my_model2.json', 'w') as json_file:\n    json_file.write(model_json)\n\n# save the model weights to an HDF5 file\nmodel.save_weights('my_model2_weights.h5')\nmodel.save('my_model2.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model.evaluate(testing_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions = model.predict(testing_generator)\n\n#Convert predicted probabilities to class labels\npredicted_labels = np.argmax(model_predictions, axis=1)\n\n#Extract true class labels from testing data\ntrue_labels = testing_generator.classes\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate confusion matrix\nmodel_confusion_matrix = confusion_matrix(true_labels, predicted_labels)\n\nprint(\"Confusion Matrix:\")\nprint(model_confusion_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting validation and training accuracy\nplt.plot(history.history['accuracy'], label='Training accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation accuracy')\nplt.title('Model accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a loss curve\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.title('Model loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using different image processing techniques.\n","metadata":{}},{"cell_type":"code","source":"# Define data training datagen parameters\ntraining_datagen = ImageDataGenerator(\n    rescale=1./255,  # Rescale pixel values\n    rotation_range=20,  # Randomly rotate image\n    height_shift_range=0.2,  # Randomly shift images vertically\n    width_shift_range=0.2,  # Randomly shift images horizontally \n    zoom_range=0.2,  # Random zoom\n    shear_range=0.2,  # Randomly shear \n    horizontal_flip=True,  # Flip horizonal\n    vertical_flip=True   # Flip vertical\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create ImageDataGenerator for training data\ntraining_generator = training_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='image_path',\n    y_col='target',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory3 = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model.evaluate(testing_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions = model.predict(testing_generator)\n\n#Convert predicted probabilities to class labels\npredicted_labels = np.argmax(model_predictions, axis=1)\n\n#Extract true class labels from testing data\ntrue_labels = testing_generator.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate confusion matrix\nmodel_confusion_matrix = confusion_matrix(true_labels, predicted_labels)\n\nprint(\"Confusion Matrix:\")\nprint(model_confusion_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels = testing_generator.labels \n\n# Calculate AUC\nauc = roc_auc_score(true_labels, model_predictions)\n\nprint(\"AUC:\", auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model weights to an HDF5 file\nmodel.save_weights('my_model3_weights.h5')\nmodel.save('my_model3.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a new model using new parameters.\n","metadata":{}},{"cell_type":"code","source":"#Changing the parameters of the model\nmodel = Sequential([\n    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),\n    MaxPooling2D(pool_size=(2, 2), strides=2),\n    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n    MaxPooling2D(pool_size=(2, 2), strides=2),\n    Flatten(),\n    Dense(units= 1, activation='sigmoid')\n])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory4 = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model.evaluate(testing_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions = model.predict(testing_generator)\n\n#Convert predicted probabilities to class labels\npredicted_labels = np.argmax(model_predictions, axis=1)\n\n#Extract true class labels from testing data\ntrue_labels = testing_generator.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate confusion matrix\nmodel_confusion_matrix = confusion_matrix(true_labels, predicted_labels)\n\nprint(\"Confusion Matrix:\")\nprint(model_confusion_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels = testing_generator.labels \n\n# Calculate AUC\nauc = roc_auc_score(true_labels, model_predictions)\n\nprint(\"AUC:\", auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model weights to an HDF5 file\nmodel.save_weights('my_model4_weights.h5')\nmodel.save('my_model4.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate class weights\nclass_weights = compute_class_weight('balanced',classes=np.unique(train_df['target']), y=train_df['target'])\n\n#Convert to dictionary\nclass_weight_dict = dict(enumerate(class_weights))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory5 = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps, class_weight = class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model weights to an HDF5 file\nmodel.save_weights('my_model5_weights.h5')\nmodel.save('my_model5.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model.evaluate(testing_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions = model.predict(testing_generator)\n\n#Convert predicted probabilities to class labels\npredicted_labels = np.argmax(model_predictions, axis=1)\n\n#Extract true class labels from testing data\ntrue_labels = testing_generator.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate confusion matrix\nmodel_confusion_matrix = confusion_matrix(true_labels, predicted_labels)\n\nprint(\"Confusion Matrix:\")\nprint(model_confusion_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels = testing_generator.labels \n\n# Calculate AUC\nauc = roc_auc_score(true_labels, model_predictions)\n\nprint(\"AUC:\", auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_labels = np.unique(train_df['target'])\nplt.bar(class_labels, class_weights)\nplt.title('Class weights')\nplt.xlabel('Class label')\nplt.ylabel('Weight')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(class_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate class frequencies\nclass_frequencies = np.bincount(train_df['target'])\n# Calculate total number of samples\ntotal_samples = len(train_df)\n# Calculate class weights\nclass_weights = total_samples / (len(class_frequencies) * class_frequencies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create heavier class weights as the first set didnt improve the model.","metadata":{}},{"cell_type":"code","source":"#Get the class weihts\nclassweight1 = class_weights[0]\nclassweight2 = class_weights[1]\n\n#Make them twice as strong for the minority class.\nclassweight1 = classweight1 / 2\nclassweight2 = classweight2 * 2\n\nclass_weights[0] = classweight1\nclass_weights[1] = classweight2\nprint(class_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_labels = np.unique(train_df['target'])\nplt.bar(class_labels, class_weights)\nplt.title('Class weights')\nplt.xlabel('Class label')\nplt.ylabel('Weight')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert to dictionary\nclass_weight_dict = dict(enumerate(class_weights))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory5 = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps, class_weight = class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define data training datagen parameters\ntraining_datagen = ImageDataGenerator(\n    rescale=1./255,  # Rescale pixel values\n    rotation_range=20,  # Randomly rotate image\n    height_shift_range=0.2,  # Randomly shift images vertically\n    width_shift_range=0.2,  # Randomly shift images horizontally \n    zoom_range=0.2,  # Random zoom\n    shear_range=0.2,  # Randomly shear \n    horizontal_flip=True,  # Flip horizonal\n    vertical_flip=True   # Flip vertical\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the oversampler and the oversampled dataframe.","metadata":{}},{"cell_type":"code","source":"# Create a RandomOverSampler object\noversampler = RandomOverSampler()\n\n# Extract the feature data (X) and target data (y) from train_df\nX = train_df['image_path']\ny = train_df['target']\n\n# Reshape X to a 2D array\nX = X.values.reshape(-1, 1)\n\n# Apply the oversampling to X and y\nX_oversampled, y_oversampled = oversampler.fit_resample(X, y)\n\n# Convert X_oversampled back to a dataframe\nX_oversampled_df = pd.DataFrame(X_oversampled, columns=['image_path'])\n\n# Concatenate X_oversampled_df with y_oversampled to get the oversampled train_df\ntrain_df_oversampled = pd.concat([X_oversampled_df, y_oversampled], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_oversampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a custom generator that takes the oversampled data as input\noversampled_train_generator = training_datagen.flow_from_dataframe(\n    dataframe=train_df_oversampled,\n    x_col='image_path', \n    y_col='target', \n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training'\n    \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df_oversampled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate the new training steps needed for the oversampled training dataframe.","metadata":{}},{"cell_type":"code","source":"#Calculate training steps\ntrain_steps = len(oversampled_train_generator) // batch_size\n\n#Calculate the validation steps\nval_steps = len(validation_generator) // batch_size\n\n#Define number of epochs\nnum_epochs = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train steps: \", train_steps)\nprint(\"Validation steps: \", val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compile a new model with the oversampled dataframe.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory6 = model.fit(oversampled_train_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the weights of the model\nmodel_weights = model.get_weights()\n\n# Loop through the list of weights to access individual weight arrays\nfor i, weight_array in enumerate(model_weights):\n    print(\"Layer {} - Weight Shape: {}\".format(i, weight_array.shape))\n    print(\"Layer {} - Weights: {}\".format(i, weight_array))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model.evaluate(testing_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions = model.predict(testing_generator)\n\n#Convert predicted probabilities to class labels\npredicted_labels = np.argmax(model_predictions, axis=1)\n\n#Extract true class labels from testing data\ntrue_labels = testing_generator.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(true_labels)\nprint(predicted_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate confusion matrix\nmodel_confusion_matrix = confusion_matrix(true_labels, predicted_labels)\n\nprint(\"Confusion Matrix:\")\nprint(model_confusion_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels = testing_generator.labels \n\n# Calculate AUC\nauc = roc_auc_score(true_labels, model_predictions)\n\nprint(\"AUC:\", auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model weights to an HDF5 file\nmodel.save_weights('my_model6_weights.h5')\nmodel.save('my_model6.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_datagen = ImageDataGenerator(\n    rescale=1./255\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a custom generator that takes the oversampled data as input\noversampled_train_generator = training_datagen.flow_from_dataframe(\n    dataframe=train_df_oversampled,\n    x_col='image_path', \n    y_col='target', \n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training'\n    \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate class frequencies\nclass_frequencies = np.bincount(train_df_oversampled['target'])\n# Calculate total number of samples\ntotal_samples = len(train_df_oversampled)\n# Calculate class weights\nclass_weights = total_samples / (len(class_frequencies) * class_frequencies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show the class weights of the oversampled dataframe they should be equal.\nclass_labels = np.unique(train_df_oversampled['target'])\nplt.bar(class_labels, class_weights)\nplt.title('Class weights')\nplt.xlabel('Class label')\nplt.ylabel('Weight')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Changing the parameters of the model\nmodel2 = Sequential([\n    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),\n    MaxPooling2D(pool_size=(2, 2), strides=2),\n    Dropout(0.5)\n    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n    MaxPooling2D(pool_size=(2, 2), strides=2),\n    Flatten(),\n    Dense(units= 1, activation='sigmoid')\n])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate training steps\ntrain_steps = len(oversampled_train_generator) // batch_size\n\n#Calculate the validation steps\nval_steps = len(validation_generator) // batch_size\n\n#Define number of epochs\nnum_epochs = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory6 = model2.fit(oversampled_train_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model weights to an HDF5 file\nmodel2.save_weights('my_model7_weights.h5')\nmodel2.save('my_model7.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model2.evaluate(testing_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions2 = model2.predict(testing_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = testing_generator.labels\n\n\nauc = roc_auc_score(y_true, model_predictions2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.5\n# Convert continuous predictions into binary labels\ny_pred_binary = np.where(model_predictions2 > threshold, 1, 0)\nprint(confusion_matrix(y_true, y_pred_binary))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Changing the parameters of the model\nmodel2 = Sequential([\n    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),\n    MaxPooling2D(pool_size=(2, 2), strides=2),\n    Dropout(0.5),\n    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n    MaxPooling2D(pool_size=(2, 2), strides=2),\n    Flatten(),\n    Dense(units= 1, activation='sigmoid')\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory6 = model2.fit(oversampled_train_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model2.evaluate(testing_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting validation and training accuracy\nplt.plot(history6.history['accuracy'], label='Training accuracy')\nplt.plot(history6.history['val_accuracy'], label='Validation accuracy')\nplt.title('Model accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a loss curve\nplt.plot(history6.history['loss'], label='Training loss')\nplt.plot(history6.history['val_loss'], label='Validation loss')\nplt.title('Model loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define model parameters\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory7 = model.fit(oversampled_train_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a new model.","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory8 = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps, class_weight = class_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recreate the training datagens with limited image pre-processing, only resizing and flipping.","metadata":{}},{"cell_type":"code","source":"# Initialize the ImageDataGenerator object\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n                            horizontal_flip=True, vertical_flip=True)\n\n#Defining batch size\nbatch_size = 32\n\n# Create an iterator for your images\ntraining_generator = datagen.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='image_path',\n    y_col='target',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training'\n)\n\n# Create an iterator for your images\nvalidation_generator = datagen.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='image_path',\n    y_col='target',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation'\n)\n\n#Create the testing generator\ntesting_generator = datagen.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='image_path',\n    y_col='target',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate training steps\ntrain_steps = len(training_generator) // batch_size\n\n#Calculate the validation steps\nval_steps = len(validation_generator) // batch_size\n\n#Define number of epochs\nnum_epochs = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define model parameters\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_df['target']), y=train_df['target'])\n\n# convert to dictionary\nclass_weight_dict = dict(enumerate(class_weights))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps,class_weight=class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model.evaluate(testing_generator, verbose=1)\n#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions = model.predict(testing_generator)\ny_true = testing_generator.labels\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = roc_auc_score(y_true, model_predictions)\n\nprint(\"AUC: \", auc)\nthreshold = 0.5\n# Convert continuous predictions into binary labels\ny_pred_binary = np.where(model_predictions > threshold, 1, 0)\nprint(confusion_matrix(y_true, y_pred_binary))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create custom loss function to prioritise the AUC.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K\n\ndef custom_loss(y_true, y_pred):\n    # Calculate binary cross-entropy loss\n    bce = K.binary_crossentropy(y_true, y_pred)\n\n    # Calculate AUC score\n    auc = K.mean(tf.keras.backend.binary_crossentropy(y_true, y_pred))\n\n    weight = 0.8 \n    loss = (1 - weight) * bce + weight * (1 - auc)\n\n    return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss=custom_loss, metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory8 = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps,class_weight=class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps,class_weight=class_weight_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_df['target']), y=train_df['target'])\n\n# convert to dictionary\nclass_weight_dict = dict(enumerate(class_weights))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps,class_weight=class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory11 = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps,class_weight=class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model5 Pretrained model with frozen weights.","metadata":{}},{"cell_type":"code","source":"    def load_pretrained_model():\n        \n        vgg_weights = '/kaggle/input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        \n        #Create the base model\n        base_model = VGG16(weights=vgg_weights, include_top=False, input_shape=image_shape, classes =2)\n    \n    \n        # freeze the first 15 layers of the base model. All other layers are trainable.\n        for layer in base_model.layers[0:15]:\n            layer.trainable = False\n\n        return base_model\n    \n    # Create a new sequentail model and add the pretrained model defined above.\n    model = Sequential()\n    model.add(load_pretrained_model())  \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20\n\nmodel.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory11 = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20\n\nmodel.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory11 = model.fit(training_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps, class_weight = class_weight_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model5 trained on the oversampled training data.","metadata":{}},{"cell_type":"code","source":"# Create a custom generator that takes the oversampled data as input\noversampled_train_generator = datagen.flow_from_dataframe(\n    dataframe=train_df_oversampled,\n    x_col='image_path', \n    y_col='target', \n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training'\n    \n)\n\n#Calculate training steps\ntrain_steps = len(oversampled_train_generator) // batch_size\n\n#Calculate the validation steps\nval_steps = len(validation_generator) // batch_size\n\n#Define number of epochs\nnum_epochs = 10\n\nmodel.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory11 = model.fit(oversampled_train_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model.evaluate(testing_generator, verbose=1)\n#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions = model.predict(testing_generator)\ny_true = testing_generator.labels\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = roc_auc_score(y_true, model_predictions)\n\nprint(\"AUC: \", auc)\nthreshold = 0.5\n# Convert continuous predictions into binary labels\ny_pred_binary = np.where(model_predictions > threshold, 1, 0)\nprint(confusion_matrix(y_true, y_pred_binary))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting validation and training accuracy\nplt.plot(history11.history['accuracy'], label='Training accuracy')\nplt.plot(history11.history['val_accuracy'], label='Validation accuracy')\nplt.title('Model accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a loss curve\nplt.plot(history11.history['auc'], label='Training loss')\nplt.plot(history11.history['val_auc'], label='Validation loss')\nplt.title('Model AUC')\nplt.xlabel('Epoch')\nplt.ylabel('AUC')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a AUC curve\nplt.plot(history11.history['loss'], label='Training loss')\nplt.plot(history11.history['val_loss'], label='Validation loss')\nplt.title('Model loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a custom generator that takes the oversampled data as input\noversampled_train_generator = datagen.flow_from_dataframe(\n    dataframe=train_df_oversampled,\n    x_col='image_path', \n    y_col='target', \n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training'\n    \n)\n\n#Calculate training steps\ntrain_steps = len(oversampled_train_generator) // batch_size\n\n#Calculate the validation steps\nval_steps = len(validation_generator) // batch_size\n\n#Define number of epochs\nnum_epochs = 30\n\n\nmodel.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n\nhistory12 = model.fit(oversampled_train_generator, steps_per_epoch=train_steps, epochs=num_epochs, \n                    validation_data=validation_generator, validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\nmodel_result = model.evaluate(testing_generator, verbose=1)\n#Output the accuracy and loss\n# Extract the loss and accuracy values from the returned values\nloss = model_result[0]\naccuracy = model_result[1]\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions = model.predict(testing_generator)\ny_true = testing_generator.labels\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = roc_auc_score(y_true, model_predictions)\n\nprint(\"AUC: \", auc)\nthreshold = 0.5\n# Convert continuous predictions into binary labels\ny_pred_binary = np.where(model_predictions > threshold, 1, 0)\nprint(confusion_matrix(y_true, y_pred_binary))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting validation and training accuracy\nplt.plot(history12.history['accuracy'], label='Training accuracy')\nplt.plot(history12.history['val_accuracy'], label='Validation accuracy')\nplt.title('Model accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a AUC curve\nplt.plot(history12.history['auc'], label='AUC accuracy')\nplt.plot(history12.history['val_auc'], label='AUC accuracy')\nplt.title('Model AUC')\nplt.xlabel('Epoch')\nplt.ylabel('AUC')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a loss curve\nplt.plot(history12.history['loss'], label='Training loss')\nplt.plot(history12.history['val_loss'], label='Validation loss')\nplt.title('Model loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}